{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partie en cours de construction : la lecture de ce chapitre a été faite, 2 problèmes vont être implémentés avec la fonction valeur et la Q-value.\n",
    "\n",
    "Or la résolution de ce problème est réalisable uniquement lorsque le chapitre 4 a été étudié, traitant de la programmation dynamique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size: Union[int, Tuple[int, int]], reward_coordinates: List[Tuple[int, int]], reward_values: List[int], teleportation_coordinates: List[Tuple[int, int]]) -> None:\n",
    "        assert len(reward_coordinates) == len(reward_values)\n",
    "        assert len(reward_coordinates) == len(teleportation_coordinates)\n",
    "\n",
    "        self.size = (size, size) if isinstance(size, int) else size\n",
    "        self.reward_coordinates = reward_coordinates\n",
    "        self.reward_values = reward_values\n",
    "        self.teleportation_coordinates = teleportation_coordinates\n",
    "\n",
    "        self.current_position = (0, 0)\n",
    "        \n",
    "        self.actions = {'top', 'bottom', 'left', 'right'}\n",
    "        self.actions_proba = {'top': 1/4, 'bottom': 1/4, 'left': 1/4, 'right': 1/4}\n",
    "        self._compute_actions = {\n",
    "            'top': (-1, 0),\n",
    "            'bottom': (1, 0),\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1)\n",
    "        }\n",
    "    \n",
    "    def is_out_of_grid(self, position: Tuple[int, int]) -> bool:\n",
    "        top_out = position[0] > self.size[0] - 1\n",
    "        bottom_out = position[0] < 0\n",
    "        right_out = position[1] > self.size[1] - 1\n",
    "        left_out = position[1] < 0\n",
    "        if top_out or bottom_out or right_out or left_out:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _is_teleportation_state(self, position: Tuple[int, int]) -> bool:\n",
    "        return position in self.reward_coordinates\n",
    "    \n",
    "    def _get_teleport_state(self, position: Tuple[int, int]) -> Tuple[int, int]:\n",
    "        teleport_idx = self.reward_coordinates.index(position)\n",
    "        return self.teleportation_coordinates[teleport_idx]\n",
    "    \n",
    "    def choose_action_from_position(self, action: str, position: Tuple[int, int]) -> Tuple[int, int]:\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f'{action} is not in available actions : {self.actions}')\n",
    "        \n",
    "        if self._is_teleportation_state(position):\n",
    "            new_position = self._get_teleport_state(position)\n",
    "        else:\n",
    "            new_position = (\n",
    "                    position[0] + self._compute_actions[action][0],\n",
    "                    position[1] + self._compute_actions[action][1]\n",
    "                )\n",
    "        \n",
    "        return new_position\n",
    "    \n",
    "    def choose_action(self, action: str) -> Tuple[int, int]:\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(f'{action} is not in available actions : {self.actions}')\n",
    "\n",
    "        new_position = (\n",
    "                self.current_position[0] + self._compute_actions[action][0],\n",
    "                self.current_position[1] + self._compute_actions[action][1]\n",
    "            )\n",
    "        \n",
    "        return new_position\n",
    "    \n",
    "    def execute_action(self, action: str) -> None:\n",
    "        new_position = self.choose_action(action)\n",
    "        if self._is_teleportation_state(new_position):\n",
    "            self.current_position = self._get_teleport_state(new_position)\n",
    "        elif not self.is_out_of_grid(new_position):\n",
    "            self.current_position = new_position\n",
    "\n",
    "    def choose_action_get_reward(self, action: str, from_position: Tuple[int, int]) -> float:\n",
    "        new_position = self.choose_action(action)\n",
    "        return self.get_state_reward(new_position, from_position)\n",
    "    \n",
    "    def get_state_reward(self, position: Tuple[int, int], from_position: Tuple[int, int]) -> float:\n",
    "        if self.is_out_of_grid(position):\n",
    "            return -1\n",
    "        elif from_position in self.reward_coordinates:\n",
    "            reward_idx = self.reward_coordinates.index(from_position)\n",
    "            return self.reward_values[reward_idx]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_current_state(self) -> Tuple[int, int]:\n",
    "        return self.current_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldAgent:\n",
    "    def __init__(self, gridworld: GridWorld, method: str, discount: float=0.1, threshold: float=None, max_iterations: int=10000) -> None:\n",
    "        self.gridworld = gridworld\n",
    "        self.method = method\n",
    "        self.states_values = torch.zeros(size=gridworld.size)\n",
    "        # for reward_position in self.gridworld.reward_coordinates:\n",
    "        #     reward_index = self.gridworld.reward_coordinates.index(reward_position)\n",
    "        #     reward_value = self.gridworld.reward_values[reward_index]\n",
    "        #     self.states_values[reward_position[0], reward_position[1]] = reward_value\n",
    "\n",
    "        self.discount = discount\n",
    "        self.threshold = threshold\n",
    "        self.max_iterations = max_iterations\n",
    "    \n",
    "    def _continue_search(self, old_values: torch.Tensor, new_values: torch.Tensor, iteration: int) -> bool:\n",
    "        if self.threshold is None:\n",
    "            threshold_reached = False\n",
    "        else:\n",
    "            threshold_reached = torch.linalg.norm((old_values - new_values).flatten(), ord=1) > self.threshold\n",
    "        return not threshold_reached and iteration < self.max_iterations\n",
    "    \n",
    "    def _update_value_function(self, value_function: torch.Tensor, row_index: int, column_index: int) -> float:\n",
    "        value = 0\n",
    "        for action in self.gridworld.actions:\n",
    "            p_a_s = self.gridworld.actions_proba[action]\n",
    "            next_state_position = self.gridworld.choose_action_from_position(action, (row_index, column_index))\n",
    "            next_state_reward = self.gridworld.get_state_reward(next_state_position, (row_index, column_index))\n",
    "            if self.gridworld.is_out_of_grid(next_state_position):\n",
    "                old_value = 0\n",
    "            else:\n",
    "                old_value = value_function[next_state_position]\n",
    "                \n",
    "            value += p_a_s * (next_state_reward + self.discount * old_value)\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def _build_policy_by_value_function(self) -> None:\n",
    "        old_values = self.states_values.clone()\n",
    "        old_values = torch.fill(old_values, float('inf'))\n",
    "        new_values = self.states_values.clone()\n",
    "        \n",
    "        iteration = 1\n",
    "        while self._continue_search(old_values, new_values, iteration):\n",
    "            old_values = new_values.clone()\n",
    "            for i in range(self.states_values.shape[0]):\n",
    "                for j in range(self.states_values.shape[1]):\n",
    "                    new_values[i, j] = self._update_value_function(old_values, i, j)\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        self.states_values = new_values.clone()\n",
    "\n",
    "    def _compute_q_value(self, position: Tuple[int, int], action: str) -> float:\n",
    "        next_position = self.gridworld.choose_action_from_position(action, position)\n",
    "        return self.gridworld.get_state_reward(next_position, position) + self.discount * self.states_values[next_position]\n",
    "\n",
    "    def choose_action_by_q_value(self, position: Tuple[int, int]) -> str:\n",
    "        possible_actions = list(self.gridworld.actions)\n",
    "        q_values = torch.zeros(size=(len(possible_actions),))\n",
    "\n",
    "        for action_idx, action in enumerate(possible_actions):\n",
    "            q_values[action_idx] = self._compute_q_value(position, action)\n",
    "\n",
    "        best_action = possible_actions[torch.argmax(q_values).item()]\n",
    "        return best_action\n",
    "    \n",
    "    def build_policy(self) -> None:\n",
    "        self._build_policy_by_value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = GridWorld(size=(5, 5), reward_coordinates=[(0, 1), (0, 3)], reward_values=[10, 5], teleportation_coordinates=[(4, 1), (2, 3)])\n",
    "agent = GridWorldAgent(gridworld=environment, method='value function', discount=0.9, threshold=None, max_iterations=10000)\n",
    "\n",
    "agent.build_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "agent.states_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.choose_action_by_q_value((0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
