{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - An Introduction\n",
    "\n",
    "Richard S. Sutton and Andrew G. Barto\n",
    "\n",
    "---\n",
    "\n",
    "## Chapitre 2\n",
    "\n",
    "Algorithme du **multi armed bandits** version **greedy**.\n",
    "\n",
    "***Notes : les autres algorithmes sont en cours d'implémentation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as distrib\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-armed bandits\n",
    "\n",
    "### Implémentation du problème de *multi armed bandits*\n",
    "\n",
    "La découverte intéressante ici est le module de distributions fourni par PyTorch. Super pratique pour générer des distributions aléatoires suivant la loi souhaitée, ou bien jouer avec les fonctions de probabilités sous-jacentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kBanditGaussian:\n",
    "    def __init__(self, k: int, *, points_loc: float=0., points_scale: float=1., reward_loc: float=0., reward_scale: float=1.) -> None:\n",
    "        self.k = k\n",
    "        self.normal_loc = points_loc\n",
    "        self.normal_scale = points_scale\n",
    "        self.reward_loc = reward_loc\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "        self._normal_point_generator = distrib.normal.Normal(torch.scalar_tensor(points_loc), torch.scalar_tensor(points_scale))\n",
    "        self._normal_reward_generator = distrib.normal.Normal(torch.scalar_tensor(reward_loc), torch.scalar_tensor(reward_scale))\n",
    "\n",
    "        self.points = self._points_initialization()\n",
    "        self.optimal_action = torch.argmax(self.points).item()\n",
    "    \n",
    "    def _points_initialization(self) -> Dict[int, float]:\n",
    "        return self._normal_point_generator.sample(sample_shape=(self.k,))\n",
    "    \n",
    "    def get_reward(self, action: int) -> float:\n",
    "        return self.points[action] + self._normal_reward_generator.sample(sample_shape=(1,)).item()\n",
    "    \n",
    "    def __call__(self, action: int) -> float:\n",
    "        return self.get_reward(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi armed bandits\n",
    "bandit_problem = kBanditGaussian(k=10, points_loc=0., points_scale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\epsilon$-greedy algorithm\n",
    "\n",
    "### Implémentation de l'algorithme greedy\n",
    "\n",
    "Plus de détails quand à l'implémentation en PyTorch pour ceux qui ne seraient pas familiers avec le *framework* :\n",
    "\n",
    "- Principe de base : \\\n",
    "Le *framework* exploite largement l'orienté objet, donc une couche de neurones ou un modèle hérite de la classe `torch.nn.Module`. Afin de définir le traitement de la couche et du modèle en général, il faut définir et implémenter la méthode `forward`.\n",
    "\n",
    "- La méthode `register_buffer` : \\\n",
    "Dans la classe héritée par `torch.nn.Module`, le *framework* reconnait automatiquement les tenseurs ou module qui sont déclarés comme étant de type (ou héritée de) `torch.nn.Parameter` ou `torch.nn.Module`. Ainsi lorsque nous désirons sauvegarder le modèle, le module génère ce qu'on appelle un *state dict* qui est un dictionnaire des paramètres du modèle, qui sont rattachés aux bons modules. Cependant pas tous les attributs déclarés dans le constructeur (`__init__`) avec la racine `self` sont pris en compte, car n'étant pas déclarés comme paramètres apprenables (`torch.nn.Parameter`). Donc, afin d'enregistrer un quelconque tenseur dans le *state dict* généré, nous pouvons faire appelle à cette méthode `register_buffer`. Une fois cette méthode appelée, l'attribut de classe est ainsi généré et nous pouvons y faire appelle via la racine `self` au sein des méthodes de la classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyBandit(nn.Module):\n",
    "    def __init__(self, epsilon: float, k: int) -> None:\n",
    "        super(GreedyBandit, self).__init__()\n",
    "        self.register_buffer('epsilon', torch.scalar_tensor(epsilon))\n",
    "        self.register_buffer('k', torch.scalar_tensor(k))\n",
    "        self.register_buffer('Q_action_values', torch.zeros(size=(k,), dtype=torch.float))\n",
    "        self.register_buffer('Q_action_values_count', torch.zeros(size=(k,), dtype=torch.int))\n",
    "\n",
    "        self._use_greedy_action_selector = distrib.categorical.Categorical(torch.tensor([1 - epsilon, epsilon]))\n",
    "        self._others_actions_selector = distrib.categorical.Categorical(torch.tensor([1/k for _ in range(k)]))\n",
    "    \n",
    "    def _select_action(self) -> int:\n",
    "        if self._use_greedy_action_selector.sample().item():\n",
    "            return self._others_actions_selector.sample().item()\n",
    "        \n",
    "        return torch.argmax(self.Q_action_values).item()\n",
    "    \n",
    "    def _update_q_action_values(self, action: int, reward: float) -> None:\n",
    "        self.Q_action_values_count[action] = self.Q_action_values_count[action] + 1\n",
    "        self.Q_action_values[action] = self.Q_action_values[action] + 1 / self.Q_action_values_count[action] * (reward - self.Q_action_values[action]) \n",
    "    \n",
    "    def forward(self, kbandit: Callable) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        action = self._select_action()\n",
    "        with torch.no_grad():\n",
    "            reward = kbandit(action)\n",
    "            self._update_q_action_values(action, reward)\n",
    "\n",
    "        return (torch.scalar_tensor(action, dtype=torch.int), torch.scalar_tensor(reward, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exécution de l'algorithme\n",
    "\n",
    "L'objectif étant de comprendre notre algorithme et de connaître son évolution. Je vous propose ici de répéter 2000 fois 1000 itérations de l'entrainement, comme proposé dans l'ouvrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline -> Sans exploration -> epsilon = 0\n",
    "baseline_histories = []\n",
    "for _ in range(2000):\n",
    "    greedy_bandit = GreedyBandit(0., 10)\n",
    "    history = []\n",
    "    for _ in range(1000):\n",
    "        history.append(greedy_bandit(bandit_problem))\n",
    "    \n",
    "    baseline_histories.append(history)\n",
    "\n",
    "baseline_histories = torch.tensor(baseline_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon-greedy\n",
    "greedy_histories = []\n",
    "for _ in range(2000):\n",
    "    greedy_bandit = GreedyBandit(0.1, 10)\n",
    "    history = []\n",
    "    for _ in range(1000):\n",
    "        history.append(greedy_bandit(bandit_problem))\n",
    "    \n",
    "    greedy_histories.append(history)\n",
    "\n",
    "greedy_histories = torch.tensor(greedy_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimistic Initial Values\n",
    "\n",
    "Dans ce cas précis, nous intitialisons nos Q-valeurs à une certaine valeur dans le but d'avoir une meilleure exploration et que toutes les actions puissent être visitées. Dans le cadre du *k-bandits*, cela marchera bien si nous trouvons une valeur supérieure à celle des moyennes des *k* distributions, nous pouvons être sûr que chacune des actions auront été exécutées au moins une fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticGreedyBandit(GreedyBandit):\n",
    "    def __init__(self, initial_value: int, epsilon: float, k: int) -> None:\n",
    "        super(OptimisticGreedyBandit, self).__init__(epsilon, k)\n",
    "\n",
    "        self.Q_action_values = torch.ones(size=(k,), dtype=torch.float) * initial_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimiscal initial values\n",
    "optimistic_greedy_histories = []\n",
    "for _ in range(2000):\n",
    "    optimistic_greedy_bandit = OptimisticGreedyBandit(5., 0., 10)\n",
    "    history = []\n",
    "    for _ in range(1000):\n",
    "        history.append(optimistic_greedy_bandit(bandit_problem))\n",
    "    \n",
    "    optimistic_greedy_histories.append(history)\n",
    "\n",
    "optimistic_greedy_histories = torch.tensor(optimistic_greedy_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper-Confidence-Bound Action selection\n",
    "\n",
    "L'objectif dans le cadre de l'*Upper-Confidence-Bound* (ou UCB) est de s'affranchir d'une probabilité d'exploration et de forcer l'exploration par le nombre de fois que l'action a été exécutée. Plus nous itérons, plus l'incertitude sur la valeur de récompense d'une action est levée. Afin d'avoir une meilleure intuition, voici la formule de séléction d'une action via UCB :\n",
    "\n",
    "$A_t = \\textrm{argmax}_a \\left[ Q_t(a) + c \\sqrt{\\frac{ln(t)}{N_t(a)}} \\right]$, avec $t$ représentant l'itération actuelle, et $N_t(a)$ le nombre de fois que l'action $a$ a été exécutée jusqu'à la $t$ ième itération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBound(nn.Module):\n",
    "    def __init__(self, c: float, k: int, eps: float=1e-7) -> None:\n",
    "        super(UpperConfidenceBound, self).__init__()\n",
    "\n",
    "        self.register_buffer('c', torch.scalar_tensor(c, dtype=torch.float))\n",
    "        self.register_buffer('k', torch.scalar_tensor(k, dtype=torch.int))\n",
    "        self.register_buffer('eps', torch.scalar_tensor(eps, dtype=torch.float))\n",
    "        self.register_buffer('Q_action_values', torch.zeros(size=(k,), dtype=torch.float))\n",
    "        self.register_buffer('Q_action_values_count', torch.zeros(size=(k,), dtype=torch.int))\n",
    "    \n",
    "    def _select_action(self, t: int) -> int:\n",
    "        current_values = self.Q_action_values + self.c * torch.sqrt(torch.log(torch.scalar_tensor(t)) / (self.Q_action_values_count + self.eps))\n",
    "        equalities = torch.where(current_values == current_values.max())[0]\n",
    "\n",
    "        if equalities.shape[0] > 1:\n",
    "           selection = distrib.categorical.Categorical(torch.tensor([1 / equalities.shape[0] for _ in range(equalities.shape[0])])).sample().item()\n",
    "           return equalities[selection]\n",
    "\n",
    "        return torch.argmax(self.Q_action_values + self.c * torch.sqrt(torch.log(torch.scalar_tensor(t)) / (self.Q_action_values_count + self.eps)))\n",
    "    \n",
    "    def _update_q_action_values(self, action: int, reward: float) -> None:\n",
    "        self.Q_action_values_count[action] = self.Q_action_values_count[action] + 1\n",
    "        self.Q_action_values[action] = self.Q_action_values[action] + 1 / self.Q_action_values_count[action] * (reward - self.Q_action_values[action])\n",
    "\n",
    "    def forward(self, kbandit: Callable,  t: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        action = self._select_action(t)\n",
    "        with torch.no_grad():\n",
    "            reward = kbandit(action)\n",
    "            self._update_q_action_values(action, reward)\n",
    "\n",
    "        return (torch.scalar_tensor(action, dtype=torch.int), torch.scalar_tensor(reward, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper Confidence Bound\n",
    "ucb_greedy_histories = []\n",
    "for _ in range(2000):\n",
    "    ucb_action_selection = UpperConfidenceBound(2., 10)\n",
    "    history = []\n",
    "    for i in range(1000):\n",
    "        history.append(ucb_action_selection(bandit_problem, i+1))\n",
    "    \n",
    "    ucb_greedy_histories.append(history)\n",
    "\n",
    "ucb_greedy_histories = torch.tensor(ucb_greedy_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Bandits Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce dernier cas, nous utilisons nous plus une valeur Q, mais une valeur de préférence. Cette préférence est déterminée par la softmax sur les valeurs de préférences de chacune des actions, définie par :\n",
    "\n",
    "$\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{i=1}^{k}H_t(b)}$ avec $H_t(a)$ la valeur de préférence de l'action $a$.\n",
    "\n",
    "La mise à jour de la valeur de préférence est régie par le gradient, je vous laisser découvrir la démonstration dans l'ouvrage. Voici la règle de mise à jour :\n",
    "\n",
    "$H_{t+1}(a) = H_t(a) + \\alpha (R_t - \\bar{R}_t)(1_{[a = A_t]} - \\pi_t(a))$ avec $\\bar{R}_t$ la moyenne des récompenses de l'action jusqu'au temps $t$ (dernier temps exclu). $\\alpha$ pouvant être assimilé au *learning rate*.\n",
    "\n",
    "Petit rappel pour réduire la gestion en mémoire et éviter d'enregistrer toutes les récompenses de toutes les actions, pour l'implémentation du calcul de $\\bar{R}_t$ :\n",
    "\n",
    "$\\bar{R}_n = \\frac{1}{n} \\sum_{i=1}^{n} R_i$\n",
    "$ = \\frac{1}{n} R_n + \\frac{1}{n} \\times \\frac{n-1}{n-1} \\sum_{i=1}^{n-1} R_i$\n",
    "$ = \\frac{1}{n} (R_n + (n-1) \\bar{R}_{n-1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBandits(nn.Module):\n",
    "    def __init__(self, alpha: float, k: int) -> None:\n",
    "        super(GradientBandits, self).__init__()\n",
    "        \n",
    "        self.register_buffer('alpha', torch.scalar_tensor(alpha, dtype=torch.float))\n",
    "        self.register_buffer('k', torch.scalar_tensor(k, dtype=torch.int))\n",
    "        self.register_buffer('preferences', torch.zeros(size=(k,)))\n",
    "        self.register_buffer('average_reward', torch.zeros(size=(k,), dtype=torch.float))\n",
    "        self.register_buffer('average_count', torch.zeros(size=(k,), dtype=torch.int))\n",
    "\n",
    "    def _select_action(self) -> None:\n",
    "        softmax_preferences = torch.softmax(self.preferences, dim=0)\n",
    "        equalities = torch.where(softmax_preferences == softmax_preferences.max())[0]\n",
    "\n",
    "        if equalities.shape[0] > 1:\n",
    "           selection = distrib.categorical.Categorical(torch.tensor([1 / equalities.shape[0] for _ in range(equalities.shape[0])])).sample().item()\n",
    "           return equalities[selection]\n",
    "\n",
    "        return torch.argmax(softmax_preferences)\n",
    "    \n",
    "    def _update_preferences(self, action: int, reward: float) -> None:\n",
    "        choice = torch.zeros(size=(self.k,), dtype=torch.float)\n",
    "        choice[action] = 1.\n",
    "\n",
    "        self.preferences = self.preferences + self.alpha * (reward - self.average_reward) * (choice - torch.softmax(self.preferences, dim=0))\n",
    "        self.average_count[action] = self.average_count[action] + 1\n",
    "        self.average_reward[action] = 1 / self.average_count[action] * (reward + (self.average_count[action] - 1) * self.average_reward[action])\n",
    "    \n",
    "    def forward(self, kbandit: Callable) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        action = self._select_action()\n",
    "        with torch.no_grad():\n",
    "            reward = kbandit(action)\n",
    "            self._update_preferences(action, reward)\n",
    "\n",
    "        return (torch.scalar_tensor(action, dtype=torch.int), torch.scalar_tensor(reward, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Bandits Algorithms\n",
    "gradient_histories = []\n",
    "for _ in range(2000):\n",
    "    gradient_bandit = GradientBandits(0.1, 10)\n",
    "    history = []\n",
    "    for _ in range(1000):\n",
    "        history.append(gradient_bandit(bandit_problem))\n",
    "    \n",
    "    gradient_histories.append(history)\n",
    "\n",
    "gradient_histories = torch.tensor(gradient_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Comme dans l'ouvrage les 2000 expérimentations du même problème nous permet d'obtenir le pourcentage d'apparition de la solution optimale à une étape donnée. Ce que je propose de faire dans la suite du *notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des pourcentages de décision optimale\n",
    "baseline_percentages = (torch.bincount(torch.where(baseline_histories[..., 0].to(torch.int) == bandit_problem.optimal_action)[1]) / baseline_histories.shape[0])\n",
    "greedy_percentages = (torch.bincount(torch.where(greedy_histories[..., 0].to(torch.int) == bandit_problem.optimal_action)[1]) / greedy_histories.shape[0])\n",
    "optimistic_percentages = (torch.bincount(torch.where(optimistic_greedy_histories[..., 0].to(torch.int) == bandit_problem.optimal_action)[1]) / optimistic_greedy_histories.shape[0])\n",
    "ucb_percentages = (torch.bincount(torch.where(ucb_greedy_histories[..., 0].to(torch.int) == bandit_problem.optimal_action)[1]) / ucb_greedy_histories.shape[0])\n",
    "gradient_percentages = (torch.bincount(torch.where(gradient_histories[..., 0].to(torch.int) == bandit_problem.optimal_action)[1]) / gradient_histories.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "ax.plot(baseline_percentages, label='0 greedy', c='blue')\n",
    "ax.plot(greedy_percentages, label='0.1 greedy', c='green')\n",
    "ax.plot(optimistic_percentages, label='optimistic initial value 5', c='red')\n",
    "ax.plot(ucb_percentages, label='UCB c=2', c='black')\n",
    "ax.plot(gradient_percentages, label='Gradient Algorithms alpha=0.1', c='orange')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_ylabel('Pourcentage apparition valeur optimale')\n",
    "ax.set_xlabel('Itération')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
